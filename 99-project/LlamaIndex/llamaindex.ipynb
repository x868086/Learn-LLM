{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade llama-index\n",
    "#!pip install llama_index.llms.openai_like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_window=3900 num_output=-1 is_chat_model=False is_function_calling_model=False model_name='deepseek-chat' system_role=<MessageRole.SYSTEM: 'system'>\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 49\u001b[0m\n\u001b[0;32m     46\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m llm\u001b[38;5;241m.\u001b[39machat(messages\u001b[38;5;241m=\u001b[39mmsgs)\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28mprint\u001b[39m(response)\n\u001b[1;32m---> 49\u001b[0m \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# asyncio.create_task(main())\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# if asyncio.get_event_loop().is_running():\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m#     # 如果没有事件循环在运行，使用 asyncio.run()\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m#     asyncio.run(main())\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\miniconda3\\Lib\\asyncio\\runners.py:186\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(main, debug)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the coroutine and return the result.\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \n\u001b[0;32m    163\u001b[0m \u001b[38;5;124;03mThis function runs the passed coroutine, taking care of\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;124;03m    asyncio.run(main())\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m events\u001b[38;5;241m.\u001b[39m_get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;66;03m# fail fast with short traceback\u001b[39;00m\n\u001b[1;32m--> 186\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    187\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Runner(debug\u001b[38;5;241m=\u001b[39mdebug) \u001b[38;5;28;01mas\u001b[39;00m runner:\n\u001b[0;32m    190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mrun(main)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.openai_like import OpenAILike\n",
    "from llama_index.core.llms import ChatMessage\n",
    "\n",
    "import asyncio\n",
    "\n",
    "llm = OpenAILike(\n",
    "    model=\"deepseek-chat\",\n",
    "    api_base=\"https://api.deepseek.com/beta\",\n",
    "    api_key=\"sk-1ed0f828e00b4dcfb8553d7796ce1321\",\n",
    ")\n",
    "\n",
    "msgs = [\n",
    "    ChatMessage(role=\"system\", content=\"你是一名历史学家，为用户讲解历史人物的故事\"),\n",
    "    ChatMessage(role=\"user\", content=\"你好\"),\n",
    "    ChatMessage(role=\"assistant\", content=\"有什么需要帮助的\"),\n",
    "    ChatMessage(role=\"user\", content=\"朱元璋是谁？有什么有趣的故事\"),\n",
    "]\n",
    "\n",
    "# 文本生成\n",
    "# response = llm.complete(\"你好，你是谁\",formatted=True)\n",
    "# print(response)\n",
    "\n",
    "# 流式文本生成\n",
    "# response = llm.stream_complete(\"徐霞客是谁\",formatted=True)\n",
    "\n",
    "# for chunk in response:\n",
    "#     print(chunk, end=' ', flush=True)\n",
    "\n",
    "\n",
    "# 对话\n",
    "# response=llm.chat(messages=msgs)\n",
    "# print(response)\n",
    "\n",
    "# 流式对话\n",
    "# response = llm.stream_chat(\n",
    "#     messages=msgs,temperature=0\n",
    "# )\n",
    "\n",
    "# for chunk in response:\n",
    "#     print(chunk, end=' ', flush=True)\n",
    "\n",
    "print(llm.metadata)\n",
    "\n",
    "# 异步流式对话\n",
    "async def main():\n",
    "    response = await llm.achat(messages=msgs)\n",
    "    print(response)\n",
    "\n",
    "\n",
    "# asyncio.create_task(main())\n",
    "\n",
    "if asyncio.get_event_loop().is_running():\n",
    "    # 如果事件循环已经在运行，创建一个新的任务\n",
    "    asyncio.create_task(main())\n",
    "else:\n",
    "    # 如果没有事件循环在运行，使用 asyncio.run()\n",
    "    asyncio.run(main())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
